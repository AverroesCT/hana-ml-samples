---
title: "HGBTDemo"
author: "PAL team"
date: "10/17/2019"
output: html_document
---

#Hybrid Gradient Boosting Tree example
Hybrid Gradient Boosting Tree in SAP HANA is a classification model based on decision trees. Gradient boosting tree (GBT) is an ensemble machine learning technique for regression and classification problems. 
Illustration of HGBT in SAP HANA by using Boston housing market dataset.


```{r setup, include=FALSE}
rm(list=ls())
library(plot3D)
knitr::opts_chunk$set(echo = TRUE,comment = NA)
```

## HANA connection

Create your own HANA instance, get a copy of BOSTON dataset and create a DSN to access HANA instance.
Parameters for the connection string (DSN,user,password):
*HANA3 : DSN to target HANA instance 
*DEVUSER: user
*Trextrex1: password
```{r Creating connection , warning= FALSE , message=FALSE}
library(hana.ml.r)
conn.context <- hanaml.ConnectionContext('HANA3','DEVUSER','Trextrex1')
```

#Load data
**The data is loaded into 2 tables, one for the training set and the other one for the validation set:**

*BOSTON_HOUSING_PRICES_TRAINING

*BOSTON_HOUSING_PRICES_TEST


## Defining datasets
```{r}
boston_train <- read.csv("../Datasets/boston-house-prices-train.csv",
                  header = FALSE,
                  col.names = c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "BLACK", "LSTAT", "MEDV", "ID"))
boston_test <- read.csv("../Datasets/boston-house-prices-test.csv",
                  header = FALSE,
                  col.names = c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "BLACK", "LSTAT", "MEDV", "ID"))
```

```{r}
ncol(boston_test)
```

```{r}
ncol(boston_train)
```

```{r DataFrame creation}
train.set <- ConvertToHANADataFrame(conn.context, boston_train, "BOSTON_HOUSING_PRICES_TRAINING", force = TRUE, native = TRUE)
test.set <- ConvertToHANADataFrame(conn.context, boston_test, "BOSTON_HOUSING_PRICES_TEST", force = TRUE, native = TRUE)

```

## Pre-processing
SAP HANA Predictive Analytics Library takes DOUBLE and INTEGER data types for most numeric types. Since we have DECIMALs and TINYINTs in our data set, we cast them to the types required by PAL.
```{r}
train.set$Head(10)$Collect()
```


```{r }
 # casting to double and integer to work with PAL.
trainDF <- train.set$cast(list("CRIM", "ZN","INDUS","NOX","RM","AGE","DIS","PTRATIO","BLACK","LSTAT","MEDV"), "DOUBLE")
trainDF <- trainDF$cast(list("CHAS","RAD","TAX"),"INTEGER")

testDF <- test.set$cast(list("CRIM", "ZN","INDUS","NOX","RM","AGE","DIS","PTRATIO","BLACK","LSTAT","MEDV"), "DOUBLE")
testDF <- testDF$cast(list("CHAS","RAD","TAX"),"INTEGER")

```

## Exploring the data
```{r Exploring the data }
sprintf("Number of rows in the train set: %s", train.set$nrows)
sprintf("Number of rows in the test set: %s", test.set$nrows)
```

## Listing all the columns from the data set  
```{r Listing all the columns from the data set, null_prefix=TRUE }
print(toString(test.set$columns))
```

* CRIM - per capita crime rate by town
* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
* INDUS - proportion of non-retail business acres per town.
* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
* NOX - nitric oxides concentration (parts per 10 million)
* RM - average number of rooms per dwelling
* AGE - proportion of owner-occupied units built prior to 1940
* DIS - weighted distances to five Boston employment centres
* RAD - index of accessibility to radial highways
* TAX - full-value property-tax rate per $10,000
* PTRATIO - pupil-teacher ratio by town
* BLACK - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
* LSTAT - % lower status of the population
* MEDV - Median value of owner-occupied homes in $1000's


## Create a HGBT model with default parameters
Calling HGBT algorithm
```{r Calling HGBT algorithm}
featurelist = list("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "BLACK", "LSTAT")
p.range <- list(n.estimators = c(4, 10, 3),
                learning.rate = c(0.1, 1.0, 3),
                split.threshold = c(0.1, 1.0, 3))
hgbtc <- hanaml.HGBTClassifier(conn.context = conn.context, data = trainDF,
                               features = featurelist,
                               label = "MEDV",
                               key = "ID",
                               learning.rate = 0.5, split.threshold = 0,
                               fold.num = 5, n.estimators = 4,
                               evaluation.metric = 'error_rate',
                               reference.metric = 'auc', max.depth = 6,
                               parameter.range = p.range,
                               random.state = 2019)
hgbtc$feature.importances$Collect()$IMPORTANCE
hgbtc$confusion.matrix$Collect()$COUNT
```

## Prediction
The second model has a better accuracy. this one will be used for the prediction.
```{r}

predict.hgbtc <-predict(hgbtc, key = "ID", data = testDF, features = featurelist, verbose = FALSE, missing.replacement = 'instance_marginalized')


sprintf("The average confidence is: %s", mean(predict.hgbtc$CONFIDENCE))
sprintf("The average score is: %s", mean(predict.hgbtc$SCORE))

```
