---
title: "RandomForestDemo"
author: "R ML library team"
output: html_document
fig_width: 10
fig_height: 8
---

#Random Forest example
Random Forest in SAP HANA is a classification model based on decision trees. Multiple decision trees are created, and the output generated are combining for a final result.
Illustration of Random Forest (random decision trees)in SAP HANA by using Boston housing market dataset.


```{r setup, include=FALSE}
rm(list=ls())
library(plot3D)
knitr::opts_chunk$set(echo = TRUE,comment = NA)
```

## HANA connection

Create your own HANA instance, get a copy of BOSTON dataset and create a DSN to access HANA instance.
Parameters for the connection string (DSN,user,password):
*HANA3 : DSN to target HANA instance 
*DEVUSER: user
*Trextrex1: password
```{r Creating connection , warning= FALSE , message=FALSE}
library(hana.ml.r)
conn.context <- hanaml.ConnectionContext('HANA3','DEVUSER','Trextrex1')
```

#Load data
**The data is loaded into 2 tables, one for the training set and the other one for the validation set:**

*BOSTON_HOUSING_PRICES_TRAINING

*BOSTON_HOUSING_PRICES_TEST


## Defining datasets

```{r dataframe creation}
train.set <- conn.context$table("BOSTON_HOUSING_PRICES_TRAINING")
test.set <- conn.context$table("BOSTON_HOUSING_PRICES_TEST")
```

## Pre-processing
SAP HANA Predictive Analytics Library takes DOUBLE and INTEGER data types for most numeric types. Since we have DECIMALs and TINYINTs in our data set, we cast them to the types required by PAL.


```{r }
 # casting to double and integer to work with PAL.
trainDF <- train.set$cast(list("CRIM", "ZN","INDUS","NOX","RM","AGE","DIS","PTRATIO","BLACK","LSTAT","MEDV"), "DOUBLE")
trainDF <- trainDF$cast(list("CHAS","RAD","TAX"),"INTEGER")

testDF <- test.set$cast(list("CRIM", "ZN","INDUS","NOX","RM","AGE","DIS","PTRATIO","BLACK","LSTAT","MEDV"), "DOUBLE")
testDF <- testDF$cast(list("CHAS","RAD","TAX"),"INTEGER")

```

## Exploring the data
```{r Exploring the data }
sprintf("Number of rows in the train set: %s", train.set$nrows)
sprintf("Number of rows in the test set: %s", test.set$nrows)
```

## Listing all the columns from the data set  
```{r Listing all the columns from the data set, null_prefix=TRUE }
print(toString(test.set$columns))
```

* CRIM - per capita crime rate by town
* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
* INDUS - proportion of non-retail business acres per town.
* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
* NOX - nitric oxides concentration (parts per 10 million)
* RM - average number of rooms per dwelling
* AGE - proportion of owner-occupied units built prior to 1940
* DIS - weighted distances to five Boston employment centres
* RAD - index of accessibility to radial highways
* TAX - full-value property-tax rate per $10,000
* PTRATIO - pupil-teacher ratio by town
* BLACK - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
* LSTAT - % lower status of the population
* MEDV - Median value of owner-occupied homes in $1000's


## Create a Random forest model with default parameters
Calling Random forest algorithm
```{r Calling Random forest algorithm}
featurelist = list("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "BLACK", "LSTAT")

random.forest<- hanaml.RandomForestClassifier(conn.context,  features = featurelist, label = "MEDV", df = trainDF, n.estimators=300, max.features=3,  random.state=2, split.threshold=0.00001,  calculate.oob=TRUE, min.samples.leaf=1, thread.ratio=1.0)

oob<- random.forest$oob.error$Collect()
sprintf("The average out of bag error, of all trees fitted, is: %s", mean(oob[[2]]))
plot(random.forest$oob.error$Collect())
```

## Create a Random forest model with the 6 random variables as candidates at each split

```{r}
random.forest2<- hanaml.RandomForestClassifier(conn.context, features = featurelist, label = "MEDV",
                                    df = trainDF,n.estimators=300, max.features=6,
                              random.state=2, split.threshold=0.00001, 
                              calculate.oob=TRUE,
                              min.samples.leaf=1, thread.ratio=1.0)

oob2<- random.forest2$oob.error$Collect()
sprintf("The average out of bag error, of all trees fitted, is: %s", mean(oob2[[2]]))
random.forest2$feature_importances_$Collect()
plot(random.forest2$oob.error$Collect())
```

The out of bag error has been reduced from **16.72** to **13.60**, when increasing the number of random variables.
The most important variables of our study is __LSTAT__ (% lower status of the population) and the second one is __RM__ (average number of rooms per dwelling).


## Prediction
The second model has a better accuracy. this one will be used for the prediction.
```{r}

predict.forest2 <-predict.RandomForestClassifier(random.forest2,key = "ID", df = testDF, features = featurelist, verbose = FALSE, block.size = 2, missing.replacement = 'instance_marginalized')


sprintf("The average confidence is: %s", mean(predict.forest2$CONFIDENCE))
sprintf("The average score is: %s", mean(predict.forest2$SCORE))

```

## What is the correct max.features(number of variables that should be randomly used for each split) for our model 

```{r}
# Using For loop to identify the right maxi.features for model
confidence=c()
score = c()

i=5

for (i in 3:13) {
random.forest3 <-  hanaml.RandomForestClassifier(conn.context, features = featurelist, label = "MEDV",
                                    df = trainDF,n.estimators=500, max.features= as.numeric(i),
                              random.state=2, split.threshold=0.00001, 
                              calculate.oob=TRUE,
                              min.samples.leaf=1, thread.ratio=1.0)
  
predict.forest3 <- predict.RandomForestClassifier(random.forest3 ,key = "ID", df = testDF, features = featurelist, verbose = FALSE, block.size = 2, missing.replacement = 'instance_marginalized')

 confidence[i-2] <- mean(predict.forest3[[3]])
}
 
plot(3:13,confidence)
```

After 6 randomly choosen variables, the confidence on the model is not hugely changed. It helps to understand here that 6 variables are suitables for the tree split. 
